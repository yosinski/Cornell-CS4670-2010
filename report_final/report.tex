\section{Introduction} % short intro

\edit{Cooper}

Our original proposal was to create a mobile application that will
identify people in an photo using pictures from facebook, and then
poll their information and status and display it in the photo.  This
was to create an augmented reality interface on the mobile device.
However, we quickly found that to create a quality application with
this ability would require more time and resources than we had.

As an alternative, we created a simpler application that utilized a
local database with the goal to find and identify faces in an mobile
image.  This database is created on a desktop computer and copied to
the phone before the application is used.  Our testing utilized two
databases: The Yale Face Database\footnote{Available here:
  \url{http://cvc.yale.edu/projects/yalefaces/yalefaces.html}}
\cite{YaleFace} and our own custom database from facebook images.

\section{Related Work} % related work, with references to papers, web pages

Face detection and identification is an active field in computer
vision, and many methods have been developed.  For face detection, one
of the most notable is the ``Viola-Jones'' method, published by Paul
Viola and Michael Jones.\cite{ViolaJones} Their work is based on
AdaBoost\cite{AdaBoost}, improved by cascading classifiers by
complexity - improving speed and accuracy.  The algorithm this project
utilizes for facial detection was developed by Rainer Lienhart and
Jochen Maydt at Intel labs.\cite{Lienhart} Their algorithm adds
rotated Harr-like features to improve accuracy.

Face identification (recognition) methods have existed for decades,
and is continually improving.  One of the oldest methods that has
stood the test of time is Eigenfaces\cite{Eigenfaces}, developed by
Matthew Turk and Alex Pentland, which utilizes Principal Component
Analysis (PCA) for recognition.  This method has been expanded using
Linear Discriminant Analysis (LDA) to improve accuracy, known as
Fisherfaces. \cite{Fisherfaces} Newer facial recognition systems use
3D models\cite{3d}, and were considered for this project, but were
ruled out because the phone has limited computational ability.

\section{Approach} % technical description including algorithm

Similar projects to ours have been conducted, such as the Introduction
to Face Detection and Face Recognition by Shervin Emami\cite{Emami}.
Our project was based on this outline in the beginning, and has grown
to include other methods and abilities.  Our project consisted of many
components, which are described in this section.

\subsection{Detection}

The first objective of this project is to idetify human faces in an
image.  Our approach was to utilize Lienhart's implementation
\cite{Lienhart} of Haar Cascades, which has already been implemented
in Intel's OpenCV library. \cite{opencv} We utilized a pretrained
cascade in the OpenCV library, which obtained acceptable accuracy and
was used in this project.

\subsection{Preprocessing}\label{preprocess}

Facial recognition, especially Eigenfaces, is very dependent on
positioning and lighting within an image.  We created a simple program
to preprocess our images for us.  It utilizes our detection algorithm
to crop the image, then it resizes the face to 100x100, then finally
converts it to grayscale and equalizes the image.  Utilizing the
detection algorithm to crop aligns the faces nearly perfectly, and
equalizing the grayscale image creates an even brightness and contrast
across all faces. These two steps are shown in
\figref{preprocess.png}.

\figvarp{preprocess.png}{.60}{Steps in our preprocessing process.}{}

This preprocess is also applied to every face found while running on
the phone.

\subsubsection{Database Generation}

We utilized two databases in this project: The Yale Face Database
\cite{Yaleface} (15 subjects, 165 images) and a database we created
from facebook of our friends (6 subjects, 70 images).  All faces are
preprocessed using the process described in \ref{preprocess}.  The
Yale database was used mostly for testing, and our database was used
in application.

Our application uses these images at run-time to generate the Fisher
Linear Discriminant models.

\subsubsection{Eigenvector Generation}

One of the two databases described above is loaded, and the
eigenvectors of the training data are generated using PCA, similar to
\cite{Eigenfaces}.  This is done beforehand, and the eigenvectors are
stored for use at application run-time.

Also, all images in the database are decomposed into the eigenvector
space, and their vectors saved to disk for nearest neighbor
calculation:

\subsection{Eigenvector Nearest Neighbor}

After a face has been preprocessed, we can identify it using the
Eigenfaces method.  This is done by projecting the face into the
eigenvector space (100x100 matrix to 1x(# of eigenvectors) vector).
Nearest neighbors is then conducted on this new vector, comparing it
to all images in the original database (their vectors are pre-calculated).

However, during testing of the Yale database, we decided to see what
would happen if one of our faces were compared (we were not in the
database).  We assumed that it would pick out the neighbors that
looked similar to us, but that's not what happened - Instead, the 10
closest neighbors were all of different subjects with similar
lighting.  This was a red flag for us, and we improved our program by
using Fisherfaces instead:

\subsection{Fisher Linear Discriminant}

% HERE

Because using nearest neighbors seemed unable to directly classify
faces correctly, we decided to use a more discriminative approach,
namely, the Fisher Linear Discriminant (FLD) \cite{wikiFld}.  FLD
attempts to find the single dimension along which data can be
projected to produce maximum \emph{separation}.  The separation $s$
can be defined as \cite{wikiFld}:

\be
s = \frac{\sigma_{between}^2}{\sigma_{within}^2}= \frac{(\vec w \cdot \vec \mu_{y=1} - \vec w \cdot \vec \mu_{y=0})^2}{\vec w^T \Sigma_{y=1} \vec w + \vec w^T \Sigma_{y=0} \vec w} = \frac{(\vec w \cdot (\vec \mu_{y=1} - \vec \mu_{y=0}))^2}{\vec w^T (\Sigma_{y=0}+\Sigma_{y=1}) \vec w}
\ee

\noindent Intuitively, we project all the data onto a vector $\vec w$,
and then choose $\vec w$ to maximize the distance between the
projected means divided by the sum of the widths of the projected
covariances.  Fortunately, the $\vec w$ that maximizes $s$ can be
calculated in closed form.

\be
\vec w = (\Sigma_{y=0}+\Sigma_{y=1})^{-1}(\vec \mu_{y=1} - \vec \mu_{y=0})
\ee

\noindent In reality, we add a normalizing term to ensure that the sum
of covariance matrices is invertible, so we calculate $\vec w$ as:

\be
\vec w = (\Sigma_{y=0} + \Sigma_{y=1} + \epsilon I)^{-1}(\vec \mu_{y=1} - \vec \mu_{y=0})
\eqlabel{w}
\ee

Finally, Fisher's Linear Discriminant is designed to maximally
separate only two classes from each other.  To work around this
limitation, we train $k$ separate one-vs-all classifiers for each of
the $k$ subjects in our database.  To classify a new test image using
the database, we compute a score for each class and then choose the
class with the highest score.  The score we chose is the following
ratio:

\be
r^{(i)} = \frac{s_{test}^{(i)} - s_{\mu_0}^{(i)}}{s_{\mu_1}^{(i)} - s_{\mu_0}^{(i)}}
\ee


\noindent where $s_{\vec x}^{(i)}$ is the projection of $\vec x$ onto $\vec w$:

\be
s_{\vec x}^{(i)} = \vec w \cdot \vec x
\ee

We could compute the FLD on the raw images, but given our 100 x 100
images we would have to invert a 10,000 by 10,000 dimensional
covariance matrix from \eqref{w} above, which would be both
prohibitively expensive and unnecessary.  Alternately, we can use PCA
to reduce the dimensionality first, and compute $\vec w$ in this space
instead.  This is the approach we ended up using.

To tune the number of eigenvectors used, we compared the performance
using different numbers of eigenvectors on classifying all the
``happy'' faces of the Yale data set.  Results are shown in
\figref{scores_vs_dim}.

\figvarp{scores_vs_dim}{.85}{Caption???}{}




\subsection{Computation Considerations for Mobiles Devices}

\edit{Cooper} 

Mobile platforms are limited in both computational power
and memory space.  Throughout this project, this was a consideration in what algorithms to use, and how our code is implemented.  To keep our application on the phone as responsive as possible, we broke op our application into three parts:

\subsubsection{Preprocessing}

We did as much computation as we could off the phone, such as
preprocessing images, generating the database, and calculating
eigenvectors.  These results were saved to files and transferred to
the phone with the application, which were loaded at run-time.

\subsubsection{Run-Time}

When the application is run it loads the database and other data files
we have already created.  The application then uses this to calculate
the Fisher discriminative features.  This causes our application to
take about 5 seconds to load, which is a reasonable amount of time.

\subsubsection{Picture}

Finally, more calculation is done on-board the phone when a picture is
taken.  First, it applies the Haar cascades to find the faces, then it
applies the preprocessing as described before. It then applies our
Fisherfaces algorithm to label the incoming face.  Finally, it paints
a square and the label onto the image, displays it on the phone, and
saves it to the phone.  All this takes about 3 seconds, which is
completely reasonable for phone-based calculations.

The Haar cascades take the longest, and originally took around 10
seconds.  The time was cut by considering less scales while applying
the cascade.

\section{Results} % experimental results

\subsection{Face Detection}


Some results using our Haar wavelet detector are shown in
\figref{small_face.jpg} through \figref{three_faces.jpg}.

\figvarp{small_face.jpg}{.60}{Haar wavelet detection of a face taking up
  a small fraction of the image.}{}

\figvarp{small_face_sideways.jpg}{.60}{Our Haar wavelet detector tends
  not to work if the faces are rotated more than 30 degrees.}{}

\figvarp{three_faces.jpg}{.60}{Detection of multiple faces at different
  scales in a single image works well.}{}

\edit{clean this up...}

\subsection{Eigenvector creation}

\edit{pull from slide}

\subsection{Eigenvector Nearest Neighbor results on Yale Data set}

\edit{Jason: Cooper is out of time}

\subsection{Fisher results on Yale Data set}

\edit{Jason}

\subsection{Fisher results on phone/computer images}

\edit{Jason}


\section{Discussion} % discussion of results, strengths/weaknesses, what worked, what didn't

\edit{Jason}


\section{Future Work} % future work and what you would do if you had more time

\edit{Jason}

Link with facebook.
add ability to realize face isnt in database.

\section{Conclusion}

\edit{Jason: can you add a few sentences as summation?}

We successfully created a working application that was able to find
and identify faces on a mobile platform.


\section{Results}




\section{Fisher Faces}

Equations from wikipedia:

